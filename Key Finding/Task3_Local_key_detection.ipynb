{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import matplotlib.style as ms\n",
    "import matplotlib.pyplot as plt\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(label_path, fileName):\n",
    "    with open(os.path.join(label_path,'REF_key_'+fileName.split('.')[-2]+'.txt')) as f:\n",
    "        \"\"\"\n",
    "        [('0', 'f'),\n",
    "         ('1', 'f'),\n",
    "         ('2', 'f')]\n",
    "        \"\"\"\n",
    "        time_list = []\n",
    "        labels = []\n",
    "        for line in f.readlines():\n",
    "            time, key = line.strip().split('\\t')\n",
    "\n",
    "            if time in time_list:\n",
    "                continue\n",
    "\n",
    "            labels.append((time,key))\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_get_features(dataset_dir, \n",
    "                      label_dir, \n",
    "                      y_source='source',\n",
    "                      gain = 10):\n",
    "\n",
    "    \n",
    "    feature_dict = {} # {song: {'window_chroma':43?, 'chroma_features':[],labels:[]}}\n",
    "\n",
    "    for dir_path, _, fileNames in os.walk(dataset_dir):\n",
    "        for fileName in fileNames:\n",
    "            print(os.path.join(dir_path,fileName))\n",
    "\n",
    "            content_dict = {'window_chroma':0, 'chroma_features':[],'labels':[]}\n",
    "            y, sr = librosa.load(os.path.join(dir_path,fileName))\n",
    "            \n",
    "            if y_source == 'harmonic':\n",
    "                    # We'll use the harmonic component to avoid pollution from transients\n",
    "                    # y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "                    y, _ = librosa.effects.hpss(y)\n",
    "                    \n",
    "            S=np.abs(librosa.stft(y))\n",
    "            C = librosa.feature.chroma_stft(S=np.log(1.0 + gain * S), sr=sr)\n",
    "\n",
    "            content_dict['chroma_features'] = C\n",
    "            content_dict['labels'] = get_labels(label_path, fileName)\n",
    "            content_dict['window_chroma'] = int(sr/512)\n",
    "\n",
    "#             print(len(content_dict['labels']))\n",
    "#             print(content_dict['window_chroma'])\n",
    "\n",
    "            feature_dict[fileName.split('.')[-2]] = content_dict\n",
    "            \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../datasets/BPS_piano'\n",
    "label_path = '../datasets/BPS_piano_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/BPS_piano/1.wav\n",
      "../datasets/BPS_piano/14.wav\n",
      "../datasets/BPS_piano/3.wav\n",
      "../datasets/BPS_piano/5.wav\n",
      "../datasets/BPS_piano/20.wav\n",
      "../datasets/BPS_piano/23.wav\n",
      "../datasets/BPS_piano/18.wav\n",
      "../datasets/BPS_piano/6.wav\n",
      "../datasets/BPS_piano/16.wav\n",
      "../datasets/BPS_piano/21.wav\n",
      "../datasets/BPS_piano/27.wav\n",
      "../datasets/BPS_piano/8.wav\n",
      "../datasets/BPS_piano/31.wav\n",
      "../datasets/BPS_piano/26.wav\n",
      "../datasets/BPS_piano/32.wav\n",
      "../datasets/BPS_piano/12.wav\n",
      "../datasets/BPS_piano/19.wav\n",
      "../datasets/BPS_piano/11.wav\n",
      "../datasets/BPS_piano/28.wav\n",
      "../datasets/BPS_piano/24.wav\n",
      "../datasets/BPS_piano/25.wav\n",
      "../datasets/BPS_piano/13.wav\n",
      "../datasets/BPS_piano/22.wav\n",
      "Features Dumped\n"
     ]
    }
   ],
   "source": [
    "feature_dict = read_get_features(dataset_dir, label_path, y_source='source',gain = 10)\n",
    "\n",
    "\n",
    "with open(\"../datasets/bps_chroma_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(feature_dict,f)\n",
    "\n",
    "print(\"Features Dumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read features from pickle\n",
    "# with open(\"../datasets/spectrum_features.pkl\",\"rb\") as f:\n",
    "#     feature_dict = pickle.load(f)\n",
    "    \n",
    "# print(\"Features Loads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Modeling\n",
    "\n",
    "\n",
    "Model the signal and tome to one second a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_windows(chroma, windows):\n",
    "    \"\"\"\n",
    "    chroma    chroma value\n",
    "    windows   time_gap, how many chroma value is one second\n",
    "    \"\"\"\n",
    "\n",
    "    length = chroma.shape[1]\n",
    "    frames_length = int(length/windows)+1\n",
    "    features = np.zeros((0,12,windows), np.float32) # (total seconds, chroma_length, windows)\n",
    "    \n",
    "    for i in range(0,length,windows):\n",
    "#         print(features.shape)\n",
    "        if i + windows < length:\n",
    "            X = chroma[:,i:i+windows]\n",
    "        else:\n",
    "            break # give up the last one\n",
    "            X = chroma[:,i:]\n",
    "            padding = windows-(length-i)\n",
    "            X_pad = np.zeros((12,padding))\n",
    "            X = np.concatenate((X, X_pad), axis=1)\n",
    "            \n",
    "        X = X.reshape(1,12,windows)\n",
    "        features = np.vstack([features, X])\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [1, 3, 5, 11, 16, 19, 20, 22, 25, 26, 32]\n",
    "valid_list = [6, 13, 14, 21, 23, 31]\n",
    "test__list = [8, 12, 18, 24, 27, 28]\n",
    "final_train_list = train_list+valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_int_dict = {'A':0,'B-':1,'B':2,'C':3,'C+':4,'D-':4,'D':5,'E-':6,\n",
    "                       'E':7,'F':8,'G-':9,'G':10,'G+':11,'A-':11, 'F+':9, 'D+':6}\n",
    "feature_to_int_dict = {**feature_to_int_dict, **{k.lower(): v+12 for k, v in feature_to_int_dict.items()}}\n",
    "# feature_to_int_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_int = np.vectorize(lambda x: (feature_to_int_dict[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(file_name_list, feature_dict):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    for i in file_name_list:\n",
    "        features = slide_windows(feature_dict[str(i)]['chroma_features'], feature_dict[str(i)]['window_chroma'])\n",
    "        for j, feature in enumerate(features):\n",
    "            feature_shape = feature.shape\n",
    "            feature = feature.reshape(-1)\n",
    "\n",
    "            try:\n",
    "                labels_list.append(feature_dict[str(i)]['labels'][j][1])\n",
    "                features_list.append(feature)\n",
    "            except:\n",
    "                print(i, j)\n",
    "                \n",
    "    return (np.array(features_list), np.array(labels_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perfect_fifth_dict = {3:10, 10:5, 5:0, 0:7, 7:2, 2:9, 9:4, 4:11, 11:6, 6:1, 1:8, 8:3, \n",
    "                      12:19,19:14, 14:21, 21:16, 16:23, 23:18, 18:13, 13:20, 20:15, 15:22, 22:17, 17:12}\n",
    "relative_dict = {3:12, 12:3, 10:19, 19:10, 5:14, 14:5, 0:21, 21:0, 7:16, 16:7, 2:23, 23:2,  \n",
    "                 9:18, 18:9, 4:13, 13:4, 11:20, 20:11, 6:15, 15:6, 1:22, 22:1, 8:17, 17:8}\n",
    "    \n",
    "def evaluation(y_true_dict, \n",
    "               y_predict_dict, \n",
    "               weighted = False, \n",
    "               to_classification_report=False, \n",
    "               target_names = [\"A\",\"A#\",\"B\",\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"a\",\"a#\",\"b\",\"c\",\"c#\",\"d\",\"d#\",\"e\",\"f\",\"f#\",\"g\",\"g#\"], \n",
    "               perfect_fifth_dict = perfect_fifth_dict, \n",
    "               relative_dict = relative_dict):\n",
    "    # classification report\n",
    "    if to_classification_report:\n",
    "        \n",
    "        print(classification_report(y_true_dict, y_predict_dict, target_names=target_names))\n",
    "        \n",
    "    class_acc = []\n",
    "    if weighted:\n",
    "        print(\"Weighted accuracy\")\n",
    "\n",
    "        correct = 0.0\n",
    "        for i, label in enumerate(y_true_dict):\n",
    "            if y_predict_dict[i] == label: # same\n",
    "                correct += 1\n",
    "            if perfect_fifth_dict[y_predict_dict[i]] == label: # perfect fifth error\n",
    "                correct += 0.5\n",
    "            if relative_dict[y_predict_dict[i]] == label: # Relative major/minor error\n",
    "                correct += 0.3\n",
    "            if (y_predict_dict[i]%12) == (label%12): # parallel major/minor\n",
    "                correct += 0.2\n",
    "        class_acc.append(correct/len(y_predict_dict))\n",
    "        print(\"{:.2f}%\".format(correct/len(y_predict_dict)*100))\n",
    "    else:\n",
    "        print(\"Average accuracy\")\n",
    "\n",
    "        correct = 0.0\n",
    "        for i, label in enumerate(y_true_dict):\n",
    "            if label == y_predict_dict[i]:\n",
    "                correct += 1\n",
    "        try:\n",
    "            class_acc.append(correct/len(y_predict_dict))\n",
    "        except:\n",
    "            class_acc.append(0.0)\n",
    "        \n",
    "        print(str(correct/len(y_predict_dict)*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 836\n",
      "(14820, 516)\n",
      "(14820,)\n",
      "32 836\n",
      "(9878, 516)\n",
      "(9878,)\n",
      "(4942, 516)\n",
      "(4942,)\n",
      "(4942, 516)\n",
      "(4942,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_train_features, all_train_labels = get_features_labels(final_train_list, feature_dict)\n",
    "\n",
    "print(all_train_features.shape)\n",
    "print(all_train_labels.shape)\n",
    "\n",
    "train_features, train_labels = get_features_labels(train_list, feature_dict)\n",
    "\n",
    "print(train_features.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "valid_features, valid_labels = get_features_labels(valid_list, feature_dict)\n",
    "print(valid_features.shape)\n",
    "print(valid_labels.shape)\n",
    "\n",
    "test_features, test_labels = get_features_labels(test__list, feature_dict)\n",
    "print(valid_features.shape)\n",
    "print(valid_labels.shape)\n",
    "\n",
    "valid_labels_int = feature_to_int(valid_labels).astype(int)\n",
    "train_labels_int = feature_to_int(train_labels).astype(int)\n",
    "test_labels_int = feature_to_int(test_labels).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build randomforest\n",
    "clf = RandomForestClassifier(random_state=42, \n",
    "                             n_estimators=128, \n",
    "                             verbose=1, \n",
    "                             n_jobs=-1)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 128 out of 128 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=128, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "clf.fit(all_train_features, feature_to_int(all_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 128 out of 128 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4929,)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ = clf.predict(test_features)\n",
    "test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          A       0.00      0.00      0.00         0\n",
      "         A#       0.16      0.28      0.21       180\n",
      "          B       0.00      0.00      0.00         0\n",
      "          C       0.36      0.10      0.15       384\n",
      "         C#       0.00      0.08      0.01        13\n",
      "          D       0.29      0.05      0.09        93\n",
      "         D#       0.42      0.39      0.40      1052\n",
      "          E       0.62      0.22      0.33       344\n",
      "          F       0.16      0.03      0.05       261\n",
      "         F#       0.00      0.00      0.00         0\n",
      "          G       0.53      0.09      0.16       445\n",
      "         G#       0.50      0.29      0.37       562\n",
      "          a       0.00      0.00      0.00         0\n",
      "         a#       0.00      0.00      0.00         1\n",
      "          b       0.06      0.19      0.09        62\n",
      "          c       0.35      0.33      0.34       763\n",
      "         c#       0.03      1.00      0.05         1\n",
      "          d       0.00      0.00      0.00        10\n",
      "         d#       0.02      0.41      0.04        17\n",
      "          e       0.34      0.09      0.15       710\n",
      "          f       0.00      0.00      0.00         1\n",
      "         f#       0.07      0.13      0.09        15\n",
      "          g       0.02      0.13      0.03        15\n",
      "\n",
      "avg / total       0.39      0.23      0.26      4929\n",
      "\n",
      "Weighted accuracy\n",
      "35.49%\n"
     ]
    }
   ],
   "source": [
    "evaluation(test_, test_labels_int, weighted = True, to_classification_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# consturct your DNN model Graph\n",
    "# The maxnumber of\n",
    "def dnn(num_hidden_uni, num_class,mode, f_dim):\n",
    "    num_hidden = len(num_hidden_uni)\n",
    "    with tf.variable_scope('dnn'):\n",
    "        # Tensor for input layer protocol\n",
    "        features = tf.placeholder(\n",
    "            tf.float32, shape=[None, f_dim], name='input_features')\n",
    "        hid = tf.layers.dense(features, num_hidden_uni[\n",
    "                             0], activation=tf.nn.relu)\n",
    "        for i in range(min(num_hidden, 5) - 1):\n",
    "            hid = tf.layers.dense(hid, num_hidden_uni[\n",
    "                                 i + 1], activation=tf.nn.relu)\n",
    "            hid = tf.layers.dropout(hid, rate=0.7, training=mode, name=\"Dropout\") # set the dropout layer and give a name \n",
    "\n",
    "        # Unscaled propability of each class\n",
    "        output_logits = tf.layers.dense(\n",
    "            hid, num_class, activation=None, name='output_layer')\n",
    "        return output_logits\n",
    "\n",
    "\n",
    "# parameters for training\n",
    "batch_size = 50\n",
    "num_epochs = 1000\n",
    "init_learning_rate = 0.00013\n",
    "epsilon = 1e-6\n",
    "num_class = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Using dataset is: ../datasets/BPS_piano\n",
      "# of epochs:  50 , valid accuracy :  0.0882 , loss: 2.00082\n",
      "# of epochs:  100 , valid accuracy :  0.1313 , loss: 1.86388\n",
      "# of epochs:  150 , valid accuracy :  0.1941 , loss: 1.78053\n",
      "# of epochs:  200 , valid accuracy :  0.2086 , loss: 1.70366\n",
      "# of epochs:  250 , valid accuracy :  0.2285 , loss: 1.79402\n",
      "# of epochs:  300 , valid accuracy :  0.2200 , loss: 1.63176\n",
      "# of epochs:  350 , valid accuracy :  0.2355 , loss: 1.93537\n",
      "# of epochs:  400 , valid accuracy :  0.2236 , loss: 1.60719\n",
      "# of epochs:  450 , valid accuracy :  0.2129 , loss: 1.52376\n",
      "# of epochs:  500 , valid accuracy :  0.2094 , loss: 1.63776\n",
      "# of epochs:  550 , valid accuracy :  0.2224 , loss: 1.38766\n",
      "# of epochs:  600 , valid accuracy :  0.2218 , loss: 1.54792\n",
      "# of epochs:  650 , valid accuracy :  0.2317 , loss: 1.18084\n",
      "# of epochs:  700 , valid accuracy :  0.2230 , loss: 1.41266\n",
      "# of epochs:  750 , valid accuracy :  0.2218 , loss: 1.40884\n",
      "# of epochs:  800 , valid accuracy :  0.2335 , loss: 1.11196\n",
      "# of epochs:  850 , valid accuracy :  0.2343 , loss: 1.15814\n",
      "# of epochs:  900 , valid accuracy :  0.2372 , loss: 1.13023\n",
      "# of epochs:  950 , valid accuracy :  0.2329 , loss: 0.785004\n",
      "# of epochs:  1000 , valid accuracy :  0.2396 , loss: 0.910284\n",
      "Finish training in 472.40 sec!\n",
      "Now test the trained DNN model....\n",
      "\n",
      "Test accuracy : 0.2402\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          A       0.03      0.21      0.05        24\n",
      "         A#       0.30      0.27      0.28       341\n",
      "          B       0.00      0.00      0.00        12\n",
      "          C       0.15      0.08      0.11       191\n",
      "         C#       0.08      0.20      0.11        89\n",
      "          D       0.47      0.05      0.09       171\n",
      "         D#       0.40      0.38      0.39      1061\n",
      "          E       0.24      0.31      0.27        96\n",
      "          F       0.14      0.02      0.04       241\n",
      "         F#       0.00      0.00      0.00         0\n",
      "          G       0.59      0.10      0.17       472\n",
      "         G#       0.57      0.28      0.38       658\n",
      "          a       0.00      0.00      0.00        18\n",
      "         a#       0.00      0.00      0.00        55\n",
      "          b       0.12      0.19      0.15       124\n",
      "          c       0.40      0.33      0.36       858\n",
      "         c#       0.00      0.00      0.00         0\n",
      "          d       0.00      0.00      0.00        10\n",
      "         d#       0.04      0.20      0.07        64\n",
      "          e       0.02      0.31      0.04        26\n",
      "          f       0.22      0.15      0.18       292\n",
      "         f#       0.04      0.09      0.05        23\n",
      "          g       0.27      0.08      0.12       103\n",
      "         g#       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.37      0.24      0.27      4929\n",
      "\n",
      "Weighted accuracy\n",
      "37.45%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        mode = tf.placeholder(tf.bool, name =\"Mode\") \n",
    "        # define your own fully connected DNN\n",
    "        output = dnn([256, 128, 128], num_class, mode,f_dim=train_features.shape[1])\n",
    "\n",
    "        # tensor for prediction the class\n",
    "        prediction = tf.argmax(output, -1)\n",
    "        # Add training ops into graph.\n",
    "        with tf.variable_scope('train'):\n",
    "            # tensor for labels\n",
    "            label_ = tf.placeholder(\n",
    "                tf.int32, shape=(None,), name='labels')\n",
    "            label = tf.one_hot(label_,depth=24)\n",
    "            \n",
    "            # tensor for calculate loss by softmax cross-entroppy\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=label,\n",
    "                logits=output,\n",
    "                name='loss_op'\n",
    "            ))\n",
    "            \n",
    "            global_step = tf.Variable(\n",
    "                0, name='global_step', trainable=False,\n",
    "                collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                             tf.GraphKeys.GLOBAL_STEP])\n",
    "            optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=init_learning_rate,\n",
    "                epsilon=epsilon)\n",
    "            train_op = optimizer.minimize(\n",
    "                loss, global_step=global_step, name='train_op')\n",
    "            arg_label = tf.argmax(label, -1)\n",
    "\n",
    "            acc = tf.reduce_mean(\n",
    "                tf.cast(tf.equal(prediction, arg_label), tf.float32), name='acc_op')\n",
    "            \n",
    "            tf.summary.scalar('cross_entropy', tf.cast(loss, tf.float32))\n",
    "            \n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "            train_writer = tf.summary.FileWriter('train_log/',\n",
    "                                                 sess.graph)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Assign the required tensors to do the operation\n",
    "        \n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'train/global_step:0')\n",
    "        features_tensor = sess.graph.get_tensor_by_name(\n",
    "            'dnn/input_features:0')\n",
    "        train_op = sess.graph.get_operation_by_name('train/train_op')\n",
    "        acc_op = sess.graph.get_tensor_by_name('train/acc_op:0')\n",
    "        \n",
    "        labels_tensor = sess.graph.get_tensor_by_name('train/labels:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('train/loss_op:0')\n",
    "        \n",
    "        # Start training\n",
    "        print('Start training...')\n",
    "        print('Using dataset is: ' + dataset_dir)\n",
    "        t0 = time()\n",
    "        epo = 0\n",
    "        while epo < num_epochs:\n",
    "            st = 0\n",
    "            for _ in range(round(train_features.shape[0] / batch_size)):\n",
    "                [num_steps, _, loss_out] = sess.run([global_step_tensor, train_op, loss_tensor], \n",
    "                                                    feed_dict={\n",
    "                                                        features_tensor: train_features[st:st + batch_size], \n",
    "                                                        labels_tensor: train_labels_int[st:st + batch_size],\n",
    "                                                        mode:True})\n",
    "                st += batch_size\n",
    "            \n",
    "#             print('loss:', '%g' % np.mean(loss_out))\n",
    "            [acc, p] = sess.run([acc_op, prediction], feed_dict={\n",
    "                features_tensor: valid_features, labels_tensor: valid_labels_int, mode:False})\n",
    "            train_writer.add_summary(summary, epo)\n",
    "            epo += 1\n",
    "            if epo%50 ==0:\n",
    "                print(\"# of epochs: \", epo,\n",
    "                      ', valid accuracy : ', '%.4f' % (acc),\n",
    "                      ', loss:', '%g' % np.mean(loss_out))\n",
    "            \n",
    "        [acc, p] = sess.run([acc_op, prediction], feed_dict={\n",
    "                features_tensor: test_features, labels_tensor: test_labels_int, mode:False})\n",
    "        print('Finish training in {:4.2f} sec!'.format(time() - t0))\n",
    "        print('Now test the trained DNN model....\\n')\n",
    "        print(\"Test accuracy : %.4f\" % (acc))\n",
    "#         print(classification_report(test_labels, p))\n",
    "        evaluation(p, test_labels_int, weighted = True, to_classification_report=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
